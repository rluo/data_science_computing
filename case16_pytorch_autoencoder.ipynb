{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T03:50:43.797510Z",
     "start_time": "2020-04-20T03:44:30.813Z"
    }
   },
   "outputs": [],
   "source": [
    "# modified from Pytorch documentation: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T22:51:43.009627Z",
     "start_time": "2020-04-19T22:51:42.679473Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T22:45:24.836015Z",
     "start_time": "2020-04-19T22:45:24.415371Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
    "usa_df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T22:45:44.254239Z",
     "start_time": "2020-04-19T22:45:44.249415Z"
    }
   },
   "outputs": [],
   "source": [
    "usa_case_df = usa_df.filter(like='/20' ) # regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T04:29:15.533808Z",
     "start_time": "2020-04-20T04:29:15.520339Z"
    }
   },
   "outputs": [],
   "source": [
    "usa_case_arr = usa_case_df.to_numpy()\n",
    "case_max = np.max(usa_case_arr)\n",
    "usa_case_arr = np.log10(1 + usa_case_arr)/np.log10(1 + case_max)\n",
    "case_mean = np.mean(  usa_case_arr )\n",
    "usa_case_arr = usa_case_arr - case_mean\n",
    "np.max(usa_case_arr), np.min(usa_case_arr)\n",
    "\n",
    "def transform(arr, arr_max, arr_mean, inverse=False):\n",
    "    if inverse:\n",
    "        ret = arr + arr_mean\n",
    "        ret = ret * np.log10(1+arr_max)\n",
    "        ret = 10**ret - 1\n",
    "    else:\n",
    "        tmp = np.log10(1+arr)/np.log10(1+arr_max)\n",
    "        ret = tmp - arr_mean\n",
    "    return ret\n",
    "    \n",
    "    \n",
    "\n",
    "day_vec = np.array( [  datetime.datetime.strptime(d, '%m/%d/%y').weekday()  for d in usa_case_df.columns ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T22:52:02.102824Z",
     "start_time": "2020-04-19T22:52:02.085293Z"
    }
   },
   "outputs": [],
   "source": [
    "class covid_dataset(Dataset):\n",
    "    def __init__(self,  arr):\n",
    "        self.arr = arr\n",
    "        no_region, no_day = arr.shape\n",
    "        self.no_region = no_region\n",
    "        self.no_day = no_day\n",
    "    def __len__(self):\n",
    "        return  self.no_region\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor( self.arr[idx,:], dtype=torch.float)\n",
    "        return x\n",
    "\n",
    "np.random.seed(2020)\n",
    "n_sample = usa_case_arr.shape[0]\n",
    "idx = np.random.permutation( n_sample )\n",
    "no_tr = round(n_sample * .6)\n",
    "no_va = round(n_sample * 0.2 )\n",
    "usa_case_arr_tr = usa_case_arr[ :no_tr , :]\n",
    "usa_case_arr_va = usa_case_arr[no_tr: (no_tr + no_va), :]\n",
    "usa_case_arr_te = usa_case_arr[(no_tr + no_va):, :]\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "\n",
    "tr_ds = covid_dataset(usa_case_arr_tr)\n",
    "va_ds = covid_dataset(usa_case_arr_va)\n",
    "te_ds = covid_dataset(usa_case_arr_te)\n",
    "tr_dl = DataLoader(tr_ds,  batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "va_dl = DataLoader(va_ds,  batch_size=len(va_ds), shuffle=False, drop_last=False)\n",
    "te_dl = DataLoader(te_ds,  batch_size=len(te_ds), shuffle=False, drop_last=False)\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tr_dl = DeviceDataLoader(tr_dl, device)\n",
    "va_dl = DeviceDataLoader(va_dl, device)\n",
    "te_dl = DeviceDataLoader(te_dl, device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T22:52:41.792160Z",
     "start_time": "2020-04-19T22:52:41.786644Z"
    }
   },
   "outputs": [],
   "source": [
    "x  = next( iter(tr_dl) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T22:52:43.102380Z",
     "start_time": "2020-04-19T22:52:43.098689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 88])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T23:47:39.899712Z",
     "start_time": "2020-04-19T23:47:39.897079Z"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T03:42:37.317868Z",
     "start_time": "2020-04-20T03:42:37.300884Z"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "seq_len = len(day_vec)\n",
    "output_dim = 1\n",
    "hidden_dim = 4\n",
    "num_layers = 1\n",
    "dropout = 0.5 if num_layers > 1 else 0\n",
    "batch_size = 1\n",
    "MAX_LENGTH = seq_len\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output =  input # 1, 1, -1\n",
    "        output, hidden = self.gru(output, hidden) # (seq_len = 1,1, 1), (1, 1, hidden_size)\n",
    "        return output, hidden #(seq_len, batch=1, input=1), (1, batch=1, hidden_size)\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(output_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden): #(1, 1,1), (1,1, hidden)\n",
    "        output = input # 1,1,-1\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden) #  (seq_len = 1,1, 1), (1, 1, hidden_size) (seq_len, batch, input_size), (num_layers * num_directions, batch, hidden_size)\n",
    "        output =  self.out(output)\n",
    "        return output, hidden  #  (seq_len, batch, num_directions * hidden_size), (num_layers * num_directions, batch, hidden_size)\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden() # (1, 1, hidden_dim)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei].unsqueeze(2), encoder_hidden) # input_tensor[ei] = 1, 1, \n",
    "        encoder_outputs[ei] = encoder_output[0, 0]  # 1 dim hidden size\n",
    "\n",
    "\n",
    "    decoder_hidden = encoder_hidden # (1, 1, hidden)\n",
    "    decoder_input = torch.tensor([[0.0]], device=device).view(1,1,1)  # (1, 1)\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di].unsqueeze(2))\n",
    "            decoder_input = target_tensor[di].unsqueeze(2) # feed reversel\n",
    "\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            decoder_input = decoder_output.detach()  # detach from history as input\n",
    "            loss += criterion(decoder_output, target_tensor[di].unsqueeze(2)).pow(2)\n",
    "#             print('decoder')\n",
    "#             print(decoder_output)\n",
    "#             print('target')\n",
    "#             print(target_tensor[di])\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item()/input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T03:42:37.551574Z",
     "start_time": "2020-04-20T03:42:37.546954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1953, 88)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choice(usa_case_arr_tr).shape\n",
    "usa_case_arr_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T03:51:19.129005Z",
     "start_time": "2020-04-20T03:51:19.117728Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000,  learning_rate=1e-5):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_tensors = [torch.tensor( random.choice(usa_case_arr_tr), dtype=torch.float).view(-1, 1, 1)\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        input_tensor = training_tensors[iter - 1] #   seq_len, 1, 1\n",
    "#         target_tensor =  input_tensor[range(-1, -len(input_tensor), -1)].clone()\n",
    "        target_tensor = input_tensor.clone()\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "#         print(loss)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T03:51:19.812578Z",
     "start_time": "2020-04-20T03:51:19.808953Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_dim, hidden_dim)\n",
    "decoder = DecoderRNN(hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T04:02:04.852241Z",
     "start_time": "2020-04-20T03:51:20.341568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 8s (- 10m 31s) (100 1%) 0.0149\n",
      "0m 16s (- 10m 18s) (200 2%) 0.0129\n",
      "0m 24s (- 9m 56s) (300 4%) 0.0084\n",
      "0m 32s (- 9m 39s) (400 5%) 0.0099\n",
      "0m 40s (- 9m 26s) (500 6%) 0.0121\n",
      "0m 48s (- 9m 15s) (600 8%) 0.0075\n",
      "0m 56s (- 9m 5s) (700 9%) 0.0091\n",
      "1m 3s (- 8m 55s) (800 10%) 0.0091\n",
      "1m 11s (- 8m 46s) (900 12%) 0.0079\n",
      "1m 20s (- 8m 40s) (1000 13%) 0.0067\n",
      "1m 28s (- 8m 34s) (1100 14%) 0.0067\n",
      "1m 36s (- 8m 25s) (1200 16%) 0.0072\n",
      "1m 44s (- 8m 18s) (1300 17%) 0.0070\n",
      "1m 52s (- 8m 10s) (1400 18%) 0.0063\n",
      "2m 0s (- 8m 2s) (1500 20%) 0.0057\n",
      "2m 8s (- 7m 54s) (1600 21%) 0.0050\n",
      "2m 17s (- 7m 47s) (1700 22%) 0.0107\n",
      "2m 25s (- 7m 40s) (1800 24%) 0.0091\n",
      "2m 33s (- 7m 31s) (1900 25%) 0.0086\n",
      "2m 41s (- 7m 23s) (2000 26%) 0.0107\n",
      "2m 49s (- 7m 15s) (2100 28%) 0.0067\n",
      "2m 57s (- 7m 7s) (2200 29%) 0.0059\n",
      "3m 5s (- 7m 0s) (2300 30%) 0.0058\n",
      "3m 13s (- 6m 52s) (2400 32%) 0.0072\n",
      "3m 21s (- 6m 43s) (2500 33%) 0.0073\n",
      "3m 30s (- 6m 36s) (2600 34%) 0.0077\n",
      "3m 39s (- 6m 29s) (2700 36%) 0.0064\n",
      "3m 47s (- 6m 21s) (2800 37%) 0.0080\n",
      "3m 55s (- 6m 13s) (2900 38%) 0.0069\n",
      "4m 4s (- 6m 6s) (3000 40%) 0.0076\n",
      "4m 13s (- 6m 0s) (3100 41%) 0.0092\n",
      "4m 25s (- 5m 56s) (3200 42%) 0.0082\n",
      "4m 35s (- 5m 50s) (3300 44%) 0.0092\n",
      "4m 45s (- 5m 44s) (3400 45%) 0.0087\n",
      "4m 55s (- 5m 38s) (3500 46%) 0.0043\n",
      "5m 5s (- 5m 30s) (3600 48%) 0.0083\n",
      "5m 14s (- 5m 22s) (3700 49%) 0.0058\n",
      "5m 22s (- 5m 14s) (3800 50%) 0.0055\n",
      "5m 31s (- 5m 5s) (3900 52%) 0.0058\n",
      "5m 39s (- 4m 57s) (4000 53%) 0.0067\n",
      "5m 47s (- 4m 48s) (4100 54%) 0.0061\n",
      "5m 56s (- 4m 40s) (4200 56%) 0.0059\n",
      "6m 5s (- 4m 31s) (4300 57%) 0.0091\n",
      "6m 14s (- 4m 23s) (4400 58%) 0.0065\n",
      "6m 22s (- 4m 14s) (4500 60%) 0.0046\n",
      "6m 31s (- 4m 6s) (4600 61%) 0.0066\n",
      "6m 40s (- 3m 58s) (4700 62%) 0.0108\n",
      "6m 49s (- 3m 50s) (4800 64%) 0.0067\n",
      "6m 57s (- 3m 41s) (4900 65%) 0.0093\n",
      "7m 7s (- 3m 33s) (5000 66%) 0.0059\n",
      "7m 17s (- 3m 25s) (5100 68%) 0.0058\n",
      "7m 25s (- 3m 17s) (5200 69%) 0.0096\n",
      "7m 34s (- 3m 8s) (5300 70%) 0.0057\n",
      "7m 43s (- 3m 0s) (5400 72%) 0.0067\n",
      "7m 52s (- 2m 51s) (5500 73%) 0.0095\n",
      "8m 1s (- 2m 43s) (5600 74%) 0.0077\n",
      "8m 10s (- 2m 34s) (5700 76%) 0.0095\n",
      "8m 18s (- 2m 26s) (5800 77%) 0.0073\n",
      "8m 27s (- 2m 17s) (5900 78%) 0.0077\n",
      "8m 35s (- 2m 8s) (6000 80%) 0.0064\n",
      "8m 44s (- 2m 0s) (6100 81%) 0.0075\n",
      "8m 53s (- 1m 51s) (6200 82%) 0.0101\n",
      "9m 1s (- 1m 43s) (6300 84%) 0.0068\n",
      "9m 9s (- 1m 34s) (6400 85%) 0.0069\n",
      "9m 18s (- 1m 25s) (6500 86%) 0.0083\n",
      "9m 27s (- 1m 17s) (6600 88%) 0.0058\n",
      "9m 35s (- 1m 8s) (6700 89%) 0.0065\n",
      "9m 44s (- 1m 0s) (6800 90%) 0.0088\n",
      "9m 52s (- 0m 51s) (6900 92%) 0.0068\n",
      "10m 1s (- 0m 42s) (7000 93%) 0.0061\n",
      "10m 9s (- 0m 34s) (7100 94%) 0.0096\n",
      "10m 18s (- 0m 25s) (7200 96%) 0.0064\n",
      "10m 27s (- 0m 17s) (7300 97%) 0.0083\n",
      "10m 36s (- 0m 8s) (7400 98%) 0.0062\n",
      "10m 44s (- 0m 0s) (7500 100%) 0.0053\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder, decoder, 7500, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T04:42:59.160452Z",
     "start_time": "2020-04-20T04:42:59.153721Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate(input_tensor, target_tensor, encoder, decoder,   max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden() # (1, 1, hidden_dim)\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei].unsqueeze(2), encoder_hidden) # input_tensor[ei] = 1, 1, \n",
    "        encoder_outputs[ei] = encoder_output[0, 0]  # 1 dim hidden size\n",
    "\n",
    "\n",
    "    decoder_hidden = encoder_hidden # (1, 1, hidden)\n",
    "    decoder_input = torch.tensor([[0.0]], device=device).view(1,1,1)  # (1, 1)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Without teacher forcing: use its own predictions as the next input\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden)\n",
    "        decoder_input = decoder_output.detach()  # detach from history as input\n",
    "        loss += criterion(decoder_output, target_tensor[di].unsqueeze(2)).pow(2)\n",
    "#     print(decoder_output)\n",
    "#     print(target_tensor[di-1])\n",
    "#             print('decoder')\n",
    "#             print(decoder_output)\n",
    "#             print('target')\n",
    "#             print(target_tensor[di])\n",
    "\n",
    "    return loss.item()/input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T04:43:50.751368Z",
     "start_time": "2020-04-20T04:42:59.571475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "va_loss = 0.0012461274160678222, te_loss=0.0010210112583835028\n"
     ]
    }
   ],
   "source": [
    "def get_decoding_err(arr):\n",
    "    va_loss = 0\n",
    "    for i in range(arr.shape[0]):\n",
    "        input_tensor = torch.tensor(  arr[i,], dtype=torch.float).view(-1, 1, 1)\n",
    "        target_tensor = input_tensor.clone()\n",
    "        va_loss += validate(input_tensor, target_tensor, encoder, decoder)\n",
    "    va_loss = va_loss/arr.shape[0]\n",
    "    return va_loss\n",
    "\n",
    "va_loss = get_decoding_err(usa_case_arr_va)\n",
    "te_loss = get_decoding_err(usa_case_arr_te)\n",
    "print(f'va_loss = {va_loss}, te_loss={te_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('pyr': conda)",
   "language": "python",
   "name": "python37364bitpyrconda45b5210c788940838c352579c0058f49"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
