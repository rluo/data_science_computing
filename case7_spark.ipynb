{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK RDD interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## local mode: pyspark --master local[7]\n",
    "## cluster mode: /usr/lib/spark/sbin/start-all.sh\n",
    "##               visit: http://google-vm-ip:8080\n",
    "##               check jobs: http://google-vm-ip:4040/jobs/\n",
    "##\t\t\t\t pyspark --master spark://slbd2017.c.slbd-157219.internal:7077 (copied from webpage)\n",
    "\n",
    "## Data from https://www.dropbox.com/s/bh5qe2ligmggw3s/sp500withAllRec.csv?dl=0\n",
    "\n",
    "## Spark v1.0+ RDD API\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = sc.textFile(\"sp500withAllRec.csv\")\n",
    "header = data.first()\n",
    "data = data.filter(lambda row: row!=header)\n",
    "parsedData = data.map(lambda line: array([float(x) for x in line.split(',')]))\n",
    "\n",
    "data.count()\n",
    "\n",
    "\n",
    "clusters = KMeans.train(parsedData, 500, maxIterations=100, initializationMode=\"random\")\n",
    "\n",
    "# load data in \"mem\" across machines\n",
    "parsedData.cache()\n",
    "clusters = KMeans.train(parsedData, 500, maxIterations=100, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get cluster labels\n",
    "parsedData.map(lambda point: clusters.predict(point)).take(10)\n",
    "parsedData.map(lambda point: clusters.predict(point)).countByValue()\n",
    "\n",
    "# compute sum squares by map-reduce\n",
    "def ss(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sum([x**2 for x in (point - center)])\n",
    "\n",
    "WSSSE = parsedData.map(lambda point: ss(point)).reduce(lambda x, y: x + y)\n",
    "print \"Within Set Sum of Squared Error = \" + str(WSSSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK 2.0 Data Frame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spark 2.0 Date Frame API \n",
    "\n",
    "spDF = spark.read.csv(\"sp500withAllRec.csv\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# first 5 lines\n",
    "spDF.limit(5).show()\n",
    "\n",
    "\n",
    "# count\n",
    "spDF.count()\n",
    "\n",
    "# pick column\n",
    "spDF.filter(spDF['aa']>0.1).show()\n",
    "spDF.filter(spDF['aa']>0.1).count()\n",
    "\n",
    "\n",
    "\n",
    "# create SQL\n",
    "spDF.createGlobalTempView(\"sp500\")\n",
    "\n",
    "\n",
    "# run sql\n",
    "spark.sql('select count(*) from global_temp.sp500').show()\n",
    "\n",
    "# cache\n",
    "spark.catalog.cacheTable(\"global_temp.sp500\")\n",
    "spark.table(\"global_temp.sp500\").count()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
